# -*- coding: utf-8 -*-
"""confussion_matrix&multiclass_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g0eJwX6rYA13_fQjO8AyPR55WjjauJ0Z

QUESTION NO 2
"""

#QUESTION 2
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Given values
TP = 2693  # True Positives
TN = 33    # True Negatives
FP = 17    # False Positives
FN = 7     # False Negatives

#Iitializing confusion matrix
confusion_matrix = np.array([[TN, FP],  # Actual No-Balls
                              [FN, TP]]) # Actual Regular Balls

#confusion matrix
plt.figure(figsize=(7, 5))
sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Reds',
            xticklabels=['Predicted NoBalls', 'Predicted Regular'],
            yticklabels=['Actual NoBalls', 'Actual Regular'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()

"""# New Section

# New Section

QUESTION NO 3
"""

#QUESTION 3
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Values for the confusion matrix
TP = 2693  # True Positives
TN = 33    # True Negatives
FP = 17    # False Positives
FN = 7     # False Negatives

# Constructing the confusion matrix
confusion_matrix = np.array([[TN, FP],
                              [FN, TP]])

# Calculating metrics
Accuracy = (TP + TN) / (TP + TN + FP + FN)
Misclassification = (FP + FN) / (TP + TN + FP + FN)
Sensitivity = TP / (TP + FN)  # True Positive Rate
FalsePositiveRate = FP / (FP + TN)  # Fall-out
Specificity = TN / (TN + FP)  # True Negative Rate
Precision = TP / (TP + FP)  # Positive Predictive Value
NegativePredictiveValue = TN / (TN + FN)  # NPV
f1Score = 2 * (Precision * Sensitivity) / (Precision + Sensitivity)  # F1 Score
MatthewsCorrelationCoefficient = ((TP * TN) - (FP * FN)) / np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))

# Output the results
print(f"Accuracy: {Accuracy:.4f}")
print(f"Misclassification Rate: {Misclassification:.4f}")
print(f"Sensitivity: {Sensitivity:.4f}")
print(f"False Positive Rate: {FalsePositiveRate:.4f}")
print(f"Specificity: {Specificity:.4f}")
print(f"Precision: {Precision:.4f}")
print(f"Negative Predictive Value: {NegativePredictiveValue:.4f}")
print(f"F1 Score: {f1Score:.4f}")
print(f"Matthews Correlation Coefficient: {MatthewsCorrelationCoefficient:.4f}")

# Plotting the confusion matrix
plt.figure(figsize=(5, 3))
sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Reds',
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()